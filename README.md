# PokeYellowAI

PokeYellowAI aims to train reinforcement learning agents to play **Pokémon Yellow** using [Gym Retro](https://github.com/openai/retro).  The repository contains scripts for extracting static game data, a goal-based reward implementation, and a simple training loop using PPO.

## Prerequisites

- Python 3.8 or newer
- [gym-retro](https://github.com/openai/retro)
- [PyTorch](https://pytorch.org/)
- A legally obtained copy of *Pokémon Yellow* placed outside this repository
This repository does not include the game ROM.

## Extracting Static Data

Some parts of the reward system rely on lookup tables for map IDs, item IDs and event flag offsets.  These can be generated by running:

```bash
python scripts/extract_static_data.py
```

The script expects `maps.asm`, `items.asm` and `ram_map.txt` from the community disassembly project to be placed in the repository root.  `ram_map.txt` should contain lines like:

```
$D747 Event Flag 0: Started the game
$D748 Event Flag 1: Received Pokédex
```

Each line begins with the flag's address followed by its description.  Running the script will create the following files under `data/`:

- `data/map_ids.json`
- `data/item_ids.json`
- `data/event_flags.json`

## Goal Definitions

Goal curricula are stored as JSON files inside the `data/` directory.  The repository provides `data/first_three_gyms.json` as an example, containing goals for reaching the first towns and defeating the first three gym leaders.

## Goal-Based Rewards

Rewards are produced by comparing consecutive WRAM snapshots.  Goals can trigger on map transitions or specific event flag bits (including badge flags).  Each goal entry contains:

- `id`: unique string identifier
- `type`: either `"map"` or `"event"`
- `target_id`: numeric map ID or flag index
- optional `reward` value and `prerequisites`

`poke_rewards.check_goals` takes two memory snapshots and returns the goals that were achieved in that frame.

Additional goal predicates are now available for item pickups and scripted events,
enabling more fine-grained progress tracking. A cumulative rewarder component
aggregates rewards over an episode so long-term goals still provide shaping
signals even when completed late in the run.

## Training

Training uses a small PPO implementation found in `train_agent.py`. The
environment is wrapped by `RomEnv` from `src/env_interface`, which exposes a
`retro`-compatible API while handling ROM resets and memory snapshots.

Place your ROM in a directory outside this repository and register it with Gym
Retro:

```bash
python -m retro.import /path/to/PokemonYellow.gbc --output integrations
```

This creates `integrations/PokemonYellow-GB` containing the ROM and metadata.
To instantiate the environment programmatically:

```python
from src.env_interface import RomEnv
import retro

retro.data.Integrations.add_custom_path("integrations")
base_env = retro.make(game="PokemonYellow-GB")
env = RomEnv(base_env)
```

You can then start training with:

```bash
python train_agent.py --retro-dir integrations --goals data/first_three_gyms.json
```

If you already imported the ROM into `~/.retro`, the `--retro-dir` argument can
be omitted. You may supply a different goals file using `--goals`. During
training, goals are unlocked gradually according to their prerequisites, forming
a simple curriculum.

